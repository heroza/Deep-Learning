{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Deep-Learning/blob/master/Skin_Cancer_Diagnosis_using_CIFAR10_Inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUusDE1Z9TNb"
      },
      "source": [
        "Prepare the dataset. \n",
        "Currently, we use skin cancer ISIC dataset from Kaggle https://www.kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic\n",
        "\n",
        "Tutorial for how to load Kaggle dataset can be found in https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "5d1529e4-ce42-4d9f-9c11-02ca2cf25a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "3effc659-935b-4020-900f-3e12f387761b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 32\n",
        "IMAGE_H = 32\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 10\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "arch = 'inception_v3'\n",
        "\n",
        "#Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_no.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=10, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images):\n",
        "  global arch\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "\t# load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "def define_base_model():\n",
        "  global arch\n",
        "  input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "  x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch == 'resnet50':\n",
        "    x = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)(x, training=False)\n",
        "  elif arch == 'inception_v3':\n",
        "    x = InceptionV3(input_shape=(224,224,3), weights='imagenet', include_top=False)(x, training=False)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tplt.subplot(211)\n",
        "\tplt.title('Cross Entropy Loss')\n",
        "\tplt.plot(history.history['loss'], color='blue', label='train')\n",
        "\tplt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tplt.subplot(212)\n",
        "\tplt.title('Classification Accuracy')\n",
        "\tplt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tplt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\tplt.close()\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train)\n",
        "  X_val = preprocess_image_input(X_val)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, IMAGE_W * IMAGE_H * 3)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4Bw4C7Fwwxad"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_cifar10_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QM00erNGU32"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "X_train, y_train, X_val, y_val, df_train, df_val = load_isic2018_dataset(train_under_frac = 0.7)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = preprocess_image_input(X_train)\n",
        "X_val = preprocess_image_input(X_val)"
      ],
      "metadata": {
        "id": "o-rGI2h3JN5s"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "ba1695e6-1977-4e1c-f5ac-dc305dadf246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpueuzs7ZBw9"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val= reset_dataset(df_train, df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8eRZiucdYnP"
      },
      "outputs": [],
      "source": [
        "#USe TF.data\n",
        "training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_data_batches = training_data.shuffle(buffer_size=40000).batch(BATCH_SIZE).prefetch(buffer_size=autotune)\n",
        "valid_data_batches = validation_data.shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(buffer_size=autotune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMBgWqIsAAB"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyGFN2-bF811",
        "outputId": "ba983b4d-6859-442e-b817-d29b159c66e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 224, 224, 3)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,430,890\n",
            "Trainable params: 24,396,458\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIygrW81Ln4z",
        "outputId": "e8a3999d-1256-4b2e-d44a-d1eb9f518d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8688 - balanced_acc: 0.8676\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.92587, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 227s 284ms/step - loss: 0.4221 - accuracy: 0.8688 - balanced_acc: 0.8676 - val_loss: 0.2167 - val_accuracy: 0.9300 - val_balanced_acc: 0.9259 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9496 - balanced_acc: 0.9478\n",
            "Epoch 2: val_balanced_acc improved from 0.92587 to 0.93233, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 283ms/step - loss: 0.1527 - accuracy: 0.9496 - balanced_acc: 0.9478 - val_loss: 0.1940 - val_accuracy: 0.9355 - val_balanced_acc: 0.9323 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9667 - balanced_acc: 0.9651\n",
            "Epoch 3: val_balanced_acc improved from 0.93233 to 0.94188, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 282ms/step - loss: 0.1013 - accuracy: 0.9667 - balanced_acc: 0.9651 - val_loss: 0.1759 - val_accuracy: 0.9446 - val_balanced_acc: 0.9419 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9780 - balanced_acc: 0.9764\n",
            "Epoch 4: val_balanced_acc improved from 0.94188 to 0.94792, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 220s 282ms/step - loss: 0.0671 - accuracy: 0.9780 - balanced_acc: 0.9764 - val_loss: 0.1599 - val_accuracy: 0.9506 - val_balanced_acc: 0.9479 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9862 - balanced_acc: 0.9848\n",
            "Epoch 5: val_balanced_acc improved from 0.94792 to 0.95068, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 283ms/step - loss: 0.0432 - accuracy: 0.9862 - balanced_acc: 0.9848 - val_loss: 0.1657 - val_accuracy: 0.9533 - val_balanced_acc: 0.9507 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9917 - balanced_acc: 0.9898\n",
            "Epoch 6: val_balanced_acc did not improve from 0.95068\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 0.0269 - accuracy: 0.9917 - balanced_acc: 0.9898 - val_loss: 0.1727 - val_accuracy: 0.9532 - val_balanced_acc: 0.9501 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9960 - balanced_acc: 0.9950\n",
            "Epoch 7: val_balanced_acc did not improve from 0.95068\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 0.0151 - accuracy: 0.9960 - balanced_acc: 0.9950 - val_loss: 0.1827 - val_accuracy: 0.9530 - val_balanced_acc: 0.9504 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9989 - balanced_acc: 0.9980\n",
            "Epoch 8: val_balanced_acc improved from 0.95068 to 0.95114, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 282ms/step - loss: 0.0064 - accuracy: 0.9989 - balanced_acc: 0.9980 - val_loss: 0.2036 - val_accuracy: 0.9536 - val_balanced_acc: 0.9511 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9997 - balanced_acc: 0.9978\n",
            "Epoch 9: val_balanced_acc improved from 0.95114 to 0.95193, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 224s 287ms/step - loss: 0.0029 - accuracy: 0.9997 - balanced_acc: 0.9978 - val_loss: 0.2115 - val_accuracy: 0.9543 - val_balanced_acc: 0.9519 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9999 - balanced_acc: 0.9990\n",
            "Epoch 10: val_balanced_acc improved from 0.95193 to 0.95196, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 283ms/step - loss: 0.0015 - accuracy: 0.9999 - balanced_acc: 0.9990 - val_loss: 0.2272 - val_accuracy: 0.9546 - val_balanced_acc: 0.9520 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 9.2305e-04 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 11: val_balanced_acc did not improve from 0.95196\n",
            "781/781 [==============================] - 220s 281ms/step - loss: 9.2305e-04 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.2363 - val_accuracy: 0.9547 - val_balanced_acc: 0.9519 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 6.5593e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 12: val_balanced_acc did not improve from 0.95196\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 6.5593e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.2430 - val_accuracy: 0.9543 - val_balanced_acc: 0.9517 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 5.1048e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 13: val_balanced_acc did not improve from 0.95196\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 5.1048e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.2513 - val_accuracy: 0.9543 - val_balanced_acc: 0.9518 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 4.2385e-04 - accuracy: 1.0000 - balanced_acc: 0.9994\n",
            "Epoch 14: val_balanced_acc did not improve from 0.95196\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 4.2385e-04 - accuracy: 1.0000 - balanced_acc: 0.9994 - val_loss: 0.2570 - val_accuracy: 0.9539 - val_balanced_acc: 0.9514 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 3.5356e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 15: val_balanced_acc improved from 0.95196 to 0.95198, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 221s 283ms/step - loss: 3.5356e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.2611 - val_accuracy: 0.9545 - val_balanced_acc: 0.9520 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 3.1027e-04 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 16: val_balanced_acc did not improve from 0.95198\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 3.1027e-04 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.2666 - val_accuracy: 0.9538 - val_balanced_acc: 0.9513 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.6863e-04 - accuracy: 1.0000 - balanced_acc: 0.9976\n",
            "Epoch 17: val_balanced_acc did not improve from 0.95198\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 2.6863e-04 - accuracy: 1.0000 - balanced_acc: 0.9976 - val_loss: 0.2707 - val_accuracy: 0.9545 - val_balanced_acc: 0.9519 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.4099e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 18: val_balanced_acc improved from 0.95198 to 0.95232, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 220s 282ms/step - loss: 2.4099e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.2747 - val_accuracy: 0.9548 - val_balanced_acc: 0.9523 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.1455e-04 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 19: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 2.1455e-04 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.2778 - val_accuracy: 0.9542 - val_balanced_acc: 0.9517 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.9356e-04 - accuracy: 1.0000 - balanced_acc: 0.9994\n",
            "Epoch 20: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.9356e-04 - accuracy: 1.0000 - balanced_acc: 0.9994 - val_loss: 0.2816 - val_accuracy: 0.9541 - val_balanced_acc: 0.9517 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.8195e-04 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 21: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.8195e-04 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.2845 - val_accuracy: 0.9536 - val_balanced_acc: 0.9512 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.6416e-04 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 22: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.6416e-04 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.2872 - val_accuracy: 0.9539 - val_balanced_acc: 0.9514 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.5205e-04 - accuracy: 1.0000 - balanced_acc: 0.9992\n",
            "Epoch 23: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.5205e-04 - accuracy: 1.0000 - balanced_acc: 0.9992 - val_loss: 0.2907 - val_accuracy: 0.9542 - val_balanced_acc: 0.9518 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3925e-04 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 24: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.3925e-04 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.2933 - val_accuracy: 0.9538 - val_balanced_acc: 0.9514 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3057e-04 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 25: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.3057e-04 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.2952 - val_accuracy: 0.9537 - val_balanced_acc: 0.9514 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2308e-04 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 26: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.2308e-04 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.2980 - val_accuracy: 0.9538 - val_balanced_acc: 0.9514 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1489e-04 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 27: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.1489e-04 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3001 - val_accuracy: 0.9541 - val_balanced_acc: 0.9518 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0873e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 28: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 1.0873e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3019 - val_accuracy: 0.9543 - val_balanced_acc: 0.9520 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0003e-04 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 29: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 1.0003e-04 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.3034 - val_accuracy: 0.9541 - val_balanced_acc: 0.9517 - lr: 5.0000e-04\n",
            "Epoch 30/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0005e-04 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 30: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 1.0005e-04 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.3044 - val_accuracy: 0.9540 - val_balanced_acc: 0.9516 - lr: 5.0000e-04\n",
            "Epoch 31/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 9.5882e-05 - accuracy: 1.0000 - balanced_acc: 0.9992\n",
            "Epoch 31: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 9.5882e-05 - accuracy: 1.0000 - balanced_acc: 0.9992 - val_loss: 0.3054 - val_accuracy: 0.9542 - val_balanced_acc: 0.9518 - lr: 5.0000e-04\n",
            "Epoch 32/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 9.2344e-05 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 32: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 9.2344e-05 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.3064 - val_accuracy: 0.9538 - val_balanced_acc: 0.9514 - lr: 5.0000e-04\n",
            "Epoch 33/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 9.2409e-05 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 33: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 9.2409e-05 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.3073 - val_accuracy: 0.9541 - val_balanced_acc: 0.9518 - lr: 5.0000e-04\n",
            "Epoch 34/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 8.9294e-05 - accuracy: 1.0000 - balanced_acc: 0.9981\n",
            "Epoch 34: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 8.9294e-05 - accuracy: 1.0000 - balanced_acc: 0.9981 - val_loss: 0.3081 - val_accuracy: 0.9539 - val_balanced_acc: 0.9516 - lr: 5.0000e-04\n",
            "Epoch 35/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 8.6829e-05 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 35: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 8.6829e-05 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3090 - val_accuracy: 0.9538 - val_balanced_acc: 0.9514 - lr: 5.0000e-04\n",
            "Epoch 36/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 8.4456e-05 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 36: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 8.4456e-05 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.3099 - val_accuracy: 0.9540 - val_balanced_acc: 0.9517 - lr: 5.0000e-04\n",
            "Epoch 37/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 8.4217e-05 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 37: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 8.4217e-05 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.3107 - val_accuracy: 0.9542 - val_balanced_acc: 0.9519 - lr: 5.0000e-04\n",
            "Epoch 38/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 8.0965e-05 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 38: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 8.0965e-05 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3117 - val_accuracy: 0.9544 - val_balanced_acc: 0.9520 - lr: 5.0000e-04\n",
            "Epoch 39/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.8988e-05 - accuracy: 1.0000 - balanced_acc: 0.9981\n",
            "Epoch 39: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 7.8988e-05 - accuracy: 1.0000 - balanced_acc: 0.9981 - val_loss: 0.3121 - val_accuracy: 0.9542 - val_balanced_acc: 0.9519 - lr: 2.5000e-04\n",
            "Epoch 40/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.9014e-05 - accuracy: 1.0000 - balanced_acc: 0.9995\n",
            "Epoch 40: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 7.9014e-05 - accuracy: 1.0000 - balanced_acc: 0.9995 - val_loss: 0.3125 - val_accuracy: 0.9541 - val_balanced_acc: 0.9518 - lr: 2.5000e-04\n",
            "Epoch 41/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.6958e-05 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 41: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 280ms/step - loss: 7.6958e-05 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.3128 - val_accuracy: 0.9539 - val_balanced_acc: 0.9517 - lr: 2.5000e-04\n",
            "Epoch 42/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.6503e-05 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 42: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.6503e-05 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3132 - val_accuracy: 0.9539 - val_balanced_acc: 0.9517 - lr: 2.5000e-04\n",
            "Epoch 43/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.5713e-05 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 43: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.5713e-05 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.3135 - val_accuracy: 0.9541 - val_balanced_acc: 0.9518 - lr: 2.5000e-04\n",
            "Epoch 44/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.4055e-05 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 44: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.4055e-05 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3140 - val_accuracy: 0.9540 - val_balanced_acc: 0.9517 - lr: 2.5000e-04\n",
            "Epoch 45/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.4329e-05 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 45: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.4329e-05 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.3145 - val_accuracy: 0.9543 - val_balanced_acc: 0.9520 - lr: 2.5000e-04\n",
            "Epoch 46/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.3658e-05 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 46: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.3658e-05 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.3148 - val_accuracy: 0.9540 - val_balanced_acc: 0.9518 - lr: 2.5000e-04\n",
            "Epoch 47/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.2443e-05 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 47: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 219s 281ms/step - loss: 7.2443e-05 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.3152 - val_accuracy: 0.9542 - val_balanced_acc: 0.9519 - lr: 2.5000e-04\n",
            "Epoch 48/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.1607e-05 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 48: val_balanced_acc did not improve from 0.95232\n",
            "781/781 [==============================] - 229s 293ms/step - loss: 7.1607e-05 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3156 - val_accuracy: 0.9542 - val_balanced_acc: 0.9519 - lr: 2.5000e-04\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vXnW3lmCgln3",
        "outputId": "6838399d-61ac-4849-a269-e232f8aafbbd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS3ewyxO_anU",
        "outputId": "5b96e362-6d38-4878-8aad-8a69ef41370c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training 1.0\n",
            "balanced accuracy on training 1.0\n",
            "accuracy on validation 0.9542\n",
            "balanced accuracy on validation 0.9541999999999999\n",
            "Score on val data:  (0.9542369334238965, 0.9541999999999999, 0.954196765905644, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3IyWjdGG4Xq"
      },
      "outputs": [],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnCoIdLyDHS"
      },
      "outputs": [],
      "source": [
        "df_train_pred = pd.DataFrame(y_train_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/20 for x in range(11)]\n",
        "\n",
        "for i in numbers:\n",
        "    df_train_pred[i]= df_train_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_train_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_train_true= [1 if x == 4 else 0 for x in np.argmax(y_train, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in num:\n",
        "    cm1 = confusion_matrix(y_train_true, df_train_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.05\n",
        "cm1 = confusion_matrix(y_train_true, df_train_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_train_pred[df_train_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_train_pred[i] == 4\n",
        "y_train_pred2 = np.where(condition, df_train_pred[i], np.argmax(y_train_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_train, axis=1), y_train_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_train, axis=1), y_train_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_train, axis=1), y_train_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60LYAT7VsNOZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIX0AmEFNv3Y"
      },
      "outputs": [],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "Y_pred2 = best_model.predict(X_test)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTEOversampling_cut-off_val-split.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "2IncA-_o_n5w",
        "outputId": "6db08704-addd-42d9-9371-4d805a2db101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Text(0, 0, 'DF'),\n",
              " Text(0, 0, 'VASC'),\n",
              " Text(0, 0, 'AKIEC'),\n",
              " Text(0, 0, 'BCC'),\n",
              " Text(0, 0, 'BKL'),\n",
              " Text(0, 0, 'MEL'),\n",
              " Text(0, 0, 'NV')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "#labels = sorted(df_train['Labels'].unique())\n",
        "#for label in range(7):\n",
        "#    plot_images_per_label(y_train, 3, (12,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(WK+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out = (H_in1)*stride[0]  2padding[0] + dilation[0](kernel_size[0]1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6qneWL_Bs2U",
        "outputId": "701f956e-871c-49f2-a88d-08d5ae5a9221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (11261, 224, 224, 3)\n",
            "Remaining Data:  (2816, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3K908bbiYwbS",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}