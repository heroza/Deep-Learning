{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heroza/Deep-Learning/blob/master/Resnet50%20on%20CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUusDE1Z9TNb"
      },
      "source": [
        "Prepare the dataset. \n",
        "Currently, we use skin cancer ISIC dataset from Kaggle https://www.kaggle.com/datasets/nodoubttome/skin-cancer9-classesisic\n",
        "\n",
        "Tutorial for how to load Kaggle dataset can be found in https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eus_4tUgfEk9",
        "outputId": "37643f50-ba27-428f-97c6-8c55fb5e313f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_x4c0_DTkaa"
      },
      "source": [
        "#Library, atribut, and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2MJBYq-oiB",
        "outputId": "543d56e9-b784-4952-b632-0cdce0268adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-c7Xghg4SB4"
      },
      "outputs": [],
      "source": [
        "# input image size\n",
        "IMAGE_W = 32\n",
        "IMAGE_H = 32\n",
        "IMG_SIZE = (IMAGE_W,IMAGE_H)\n",
        "num_classes = 10\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "opt_adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "opt_SGD = SGD(learning_rate=0.001)\n",
        "arch = 'resnet50'\n",
        "\n",
        "#Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2), \n",
        "  layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n",
        "  layers.experimental.preprocessing.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n",
        "])\n",
        "\n",
        "#Callbacks\n",
        "best_model_fpath = '/content/drive/MyDrive/PHD/Model/best_model_no.h5'\n",
        "last_model_fpath = '/content/drive/MyDrive/PHD/Model/last_model_no.h5'\n",
        "mc = ModelCheckpoint(best_model_fpath, monitor='val_balanced_acc', mode='max', verbose=1, save_best_only=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_balanced_acc', patience=10, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "early_stopping_monitor = EarlyStopping(patience=30,monitor='val_balanced_acc')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JffFid9sOXeo"
      },
      "outputs": [],
      "source": [
        "# load train and test dataset\n",
        "def preprocess_image_input(input_images):\n",
        "  global arch\n",
        "  input_images = input_images.astype('float32')\n",
        "  if arch == 'inception_v3':\n",
        "    output_ims = tf.keras.applications.inception_v3.preprocess_input(input_images)\n",
        "  else:\n",
        "    output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n",
        "\n",
        "def load_cifar10_dataset():\n",
        "  from keras.datasets import cifar10\n",
        "    # load dataset\n",
        "  (X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def balanced_acc(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "\n",
        "    tensor1 = tf.math.argmax(y_true, axis=1)\n",
        "    tensor2 = tf.math.argmax(y_pred, axis=1)\n",
        "\n",
        "    cm = tf.math.confusion_matrix(tensor1, tensor2)\n",
        "    \n",
        "    diag = tf.linalg.tensor_diag_part (cm)\n",
        "    tpfn = tf.cast(K.sum(cm, axis = 1), tf.float32) + K.epsilon()\n",
        "    recall = tf.divide(tf.cast(diag, tf.float32),tpfn)\n",
        "    balanced_acc = K.mean(recall)\n",
        "    return balanced_acc\n",
        "\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_base_model():\n",
        "  global arch\n",
        "  input_tensor = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "  x = UpSampling2D(size=(7,7))(input_tensor)\n",
        "  #x = data_augmentation(input_tensor)\n",
        "  #x = layers.Rescaling(1.0 / 255)(input_tensor)  # Rescale inputs\n",
        "  if arch == 'resnet50':\n",
        "    x = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)(x, training=False)\n",
        "  elif arch == 'inception_v3':\n",
        "    x = InceptionV3(input_shape=(224,224,3), weights='imagenet', include_top=False)(x, training=False)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  predictions = Dense(num_classes, activation='softmax')(x)\n",
        "  model = Model(inputs=input_tensor, outputs=predictions)\n",
        "  model.compile(optimizer = opt_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "  return model\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "    # plot loss\n",
        "    plt.subplot(211)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    # plot accuracy\n",
        "    plt.subplot(212)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    plt.close()\n",
        " \n",
        "# scale pixels\n",
        "def norm_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "def load_isic2018_dataset(train_under_frac = 0):\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_GroundTruth/ISIC2018_Task3_Training_GroundTruth.csv') \n",
        "  df_val = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_GroundTruth/ISIC2018_Task3_Validation_GroundTruth.csv') \n",
        "\n",
        "  #decode one hot label\n",
        "  df_train[\"Labels\"] = (df_train.iloc[:, 1:]).idxmax(axis=1)\n",
        "  df_val[\"Labels\"] = (df_val.iloc[:, 1:]).idxmax(axis=1)\n",
        "\n",
        "  #random undersampling for training dataset\n",
        "  if train_under_frac !=0:\n",
        "    df_train = df_train.drop(df_train[df_train['Labels'] == 'NV'].sample(frac=train_under_frac).index)\n",
        "\n",
        "  #drop one-hot column\n",
        "  df_train = df_train.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "  df_val = df_val.drop(columns=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'])\n",
        "\n",
        "  #make filepaths of the image\n",
        "  dir_train = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_Input/'\n",
        "  dir_val = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Validation_Input/'\n",
        "  df_train['FilePaths'] = dir_train + df_train['image'] + '.jpg'\n",
        "  df_val['FilePaths'] = dir_val + df_val['image'] + '.jpg'\n",
        "  \n",
        "  #load image pixels to dataframe\n",
        "  df_train['image_px'] = df_train['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "  df_val['image_px'] = df_val['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))\n",
        "\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, df_train, df_val\n",
        "\n",
        "def reset_dataset(df_train, df_val):\n",
        "  X_train = np.asarray(df_train['image_px'].tolist())\n",
        "  X_val = np.asarray(df_val['image_px'].tolist())\n",
        "  y_train = np.array(df_train['Labels'].values)\n",
        "  y_val = np.array(df_val['Labels'].values)\n",
        "\n",
        "  X_train = preprocess_image_input(X_train)\n",
        "  X_val = preprocess_image_input(X_val)\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  y_train = label_encoder.fit_transform(y_train)\n",
        "  y_val = label_encoder.fit_transform(y_val)\n",
        "  \n",
        "  y_train = to_categorical(y_train, num_classes = num_classes)\n",
        "  y_val = to_categorical(y_val, num_classes = num_classes)\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "def SMOTE_Data(X, y, one_hot = False, k = 5):\n",
        "  if one_hot:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "  X_resampled, y_resampled = sm.fit_resample(X.reshape((-1, IMAGE_W * IMAGE_H * 3)), y)\n",
        "  X_resampled = X_resampled.reshape(-1, IMAGE_W, IMAGE_H, 3)\n",
        "  if one_hot:\n",
        "    y_resampled = to_categorical(y_resampled, num_classes = num_classes)\n",
        "  else:\n",
        "    y_resampled = y_resampled.reshape(-1,1)\n",
        "  return X_resampled, y_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7sLC2svMuJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bw4C7Fwwxad"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_cifar10_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QM00erNGU32",
        "outputId": "48578447-73ea-4d62-ab07-5cf9971d8d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5321, 224, 224, 3)\n",
            "(5321, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "X_train, y_train, X_val, y_val, df_train, df_val = load_isic2018_dataset(train_under_frac = 0.7)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-rGI2h3JN5s"
      },
      "outputs": [],
      "source": [
        "X_train = preprocess_image_input(X_train)\n",
        "X_val = preprocess_image_input(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArGWuciBt_-",
        "outputId": "6cd7baf5-ad30-470f-a4d2-0d0f23b7652f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = SMOTE_Data(X_train, y_train, True)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8eRZiucdYnP"
      },
      "outputs": [],
      "source": [
        "#USe TF.data\n",
        "training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_data_batches = training_data.shuffle(buffer_size=40000).batch(BATCH_SIZE).prefetch(buffer_size=autotune)\n",
        "valid_data_batches = validation_data.shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(buffer_size=autotune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Z_nccu6QjB"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.DataFrame(X_train.reshape(X_train.shape[0],-1))\n",
        "df1['y_train'] = np.argmax(y_train, axis=1).tolist()\n",
        "df2 = pd.DataFrame(X_val.reshape(X_val.shape[0],-1))\n",
        "df2['y_val'] = np.argmax(y_val, axis=1).tolist()\n",
        "df1.to_pickle(path+\"isic2018_SMOTE_train.pkl\")\n",
        "df2.to_pickle(path+\"isic2018_SMOTE_val.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qge6cnxQPnH6"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/PHD/Datasets/isic2018/'\n",
        "df1 = pd.read_pickle(path+\"isic2018_SMOTE_train.pkl\")\n",
        "X_train = df1.loc[:, df1.columns != 'y_train'].to_numpy()\n",
        "X_train = X_train.reshape(-1,224,224,3)\n",
        "y_train = df1.loc[:, df1.columns == 'y_train'].to_numpy()\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "df1 = pd.read_pickle(path+\"isic2018_SMOTE_val.pkl\")\n",
        "X_val = df1.loc[:, df1.columns != 'y_val'].to_numpy()\n",
        "X_val = X_val.reshape(-1,224,224,3)\n",
        "y_val = df1.loc[:, df1.columns == 'y_val'].to_numpy()\n",
        "y_val = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMBgWqIsAAB",
        "outputId": "1c84b507-8d13-45fa-b514-dc3a4c250bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14077, 224, 224, 3)\n",
            "(14077, 7)\n",
            "(193, 224, 224, 3)\n",
            "(193, 7)\n",
            "Counter train data:  Counter({5: 2011, 4: 2011, 2: 2011, 3: 2011, 0: 2011, 1: 2011, 6: 2011})\n",
            "Counter val data:  Counter({5: 123, 2: 22, 4: 21, 1: 15, 0: 8, 6: 3, 3: 1})\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print('Counter train data: ', Counter(np.argmax(y_train, axis=1)))\n",
        "print('Counter val data: ', Counter(np.argmax(y_val, axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIygrW81Ln4z",
        "outputId": "70bdbb48-05df-4ab6-90ce-72a34d03c7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.8165 - balanced_acc: 0.8153\n",
            "Epoch 1: val_balanced_acc improved from -inf to 0.88730, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 299s 361ms/step - loss: 0.5360 - accuracy: 0.8165 - balanced_acc: 0.8153 - val_loss: 0.3077 - val_accuracy: 0.8925 - val_balanced_acc: 0.8873 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9324 - balanced_acc: 0.9313\n",
            "Epoch 2: val_balanced_acc improved from 0.88730 to 0.90892, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 279s 357ms/step - loss: 0.2002 - accuracy: 0.9324 - balanced_acc: 0.9313 - val_loss: 0.2629 - val_accuracy: 0.9118 - val_balanced_acc: 0.9089 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9551 - balanced_acc: 0.9542\n",
            "Epoch 3: val_balanced_acc improved from 0.90892 to 0.93449, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 279s 357ms/step - loss: 0.1321 - accuracy: 0.9551 - balanced_acc: 0.9542 - val_loss: 0.1945 - val_accuracy: 0.9359 - val_balanced_acc: 0.9345 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9696 - balanced_acc: 0.9683\n",
            "Epoch 4: val_balanced_acc improved from 0.93449 to 0.93940, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 278s 357ms/step - loss: 0.0917 - accuracy: 0.9696 - balanced_acc: 0.9683 - val_loss: 0.1913 - val_accuracy: 0.9413 - val_balanced_acc: 0.9394 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9785 - balanced_acc: 0.9769\n",
            "Epoch 5: val_balanced_acc did not improve from 0.93940\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 0.0644 - accuracy: 0.9785 - balanced_acc: 0.9769 - val_loss: 0.1886 - val_accuracy: 0.9413 - val_balanced_acc: 0.9392 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9868 - balanced_acc: 0.9856\n",
            "Epoch 6: val_balanced_acc improved from 0.93940 to 0.94088, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 278s 356ms/step - loss: 0.0400 - accuracy: 0.9868 - balanced_acc: 0.9856 - val_loss: 0.1989 - val_accuracy: 0.9433 - val_balanced_acc: 0.9409 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9934 - balanced_acc: 0.9927\n",
            "Epoch 7: val_balanced_acc improved from 0.94088 to 0.94194, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 277s 355ms/step - loss: 0.0241 - accuracy: 0.9934 - balanced_acc: 0.9927 - val_loss: 0.2112 - val_accuracy: 0.9441 - val_balanced_acc: 0.9419 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9967 - balanced_acc: 0.9960\n",
            "Epoch 8: val_balanced_acc did not improve from 0.94194\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 0.0136 - accuracy: 0.9967 - balanced_acc: 0.9960 - val_loss: 0.2716 - val_accuracy: 0.9364 - val_balanced_acc: 0.9337 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9991 - balanced_acc: 0.9981\n",
            "Epoch 9: val_balanced_acc improved from 0.94194 to 0.94377, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 277s 355ms/step - loss: 0.0066 - accuracy: 0.9991 - balanced_acc: 0.9981 - val_loss: 0.2442 - val_accuracy: 0.9470 - val_balanced_acc: 0.9438 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9999 - balanced_acc: 0.9989\n",
            "Epoch 10: val_balanced_acc improved from 0.94377 to 0.94462, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 277s 354ms/step - loss: 0.0030 - accuracy: 0.9999 - balanced_acc: 0.9989 - val_loss: 0.2558 - val_accuracy: 0.9470 - val_balanced_acc: 0.9446 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000 - balanced_acc: 0.9989\n",
            "Epoch 11: val_balanced_acc did not improve from 0.94462\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 0.0016 - accuracy: 1.0000 - balanced_acc: 0.9989 - val_loss: 0.2713 - val_accuracy: 0.9460 - val_balanced_acc: 0.9436 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 12: val_balanced_acc did not improve from 0.94462\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 0.0011 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.2842 - val_accuracy: 0.9468 - val_balanced_acc: 0.9441 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 7.8880e-04 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 13: val_balanced_acc did not improve from 0.94462\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 7.8880e-04 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.2946 - val_accuracy: 0.9470 - val_balanced_acc: 0.9442 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 6.2529e-04 - accuracy: 1.0000 - balanced_acc: 0.9991\n",
            "Epoch 14: val_balanced_acc improved from 0.94462 to 0.94492, saving model to /content/drive/MyDrive/PHD/Model/best_model_no.h5\n",
            "781/781 [==============================] - 277s 355ms/step - loss: 6.2529e-04 - accuracy: 1.0000 - balanced_acc: 0.9991 - val_loss: 0.3022 - val_accuracy: 0.9474 - val_balanced_acc: 0.9449 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 5.1030e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 15: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 5.1030e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3099 - val_accuracy: 0.9464 - val_balanced_acc: 0.9436 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 4.3705e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 16: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 4.3705e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.3153 - val_accuracy: 0.9470 - val_balanced_acc: 0.9444 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 3.7919e-04 - accuracy: 1.0000 - balanced_acc: 0.9986\n",
            "Epoch 17: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 3.7919e-04 - accuracy: 1.0000 - balanced_acc: 0.9986 - val_loss: 0.3222 - val_accuracy: 0.9471 - val_balanced_acc: 0.9441 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 3.3578e-04 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 18: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 352ms/step - loss: 3.3578e-04 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3273 - val_accuracy: 0.9472 - val_balanced_acc: 0.9444 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.9609e-04 - accuracy: 1.0000 - balanced_acc: 0.9981\n",
            "Epoch 19: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 2.9609e-04 - accuracy: 1.0000 - balanced_acc: 0.9981 - val_loss: 0.3314 - val_accuracy: 0.9471 - val_balanced_acc: 0.9443 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.6939e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 20: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 353ms/step - loss: 2.6939e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3361 - val_accuracy: 0.9471 - val_balanced_acc: 0.9442 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.4187e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 21: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 2.4187e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3404 - val_accuracy: 0.9473 - val_balanced_acc: 0.9443 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.2444e-04 - accuracy: 1.0000 - balanced_acc: 0.9992\n",
            "Epoch 22: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 2.2444e-04 - accuracy: 1.0000 - balanced_acc: 0.9992 - val_loss: 0.3436 - val_accuracy: 0.9474 - val_balanced_acc: 0.9446 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.0755e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 23: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 2.0755e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3468 - val_accuracy: 0.9470 - val_balanced_acc: 0.9445 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.8759e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 24: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.8759e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.3504 - val_accuracy: 0.9471 - val_balanced_acc: 0.9444 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.7670e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 25: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.7670e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3521 - val_accuracy: 0.9469 - val_balanced_acc: 0.9441 - lr: 5.0000e-04\n",
            "Epoch 26/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.6773e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 26: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 352ms/step - loss: 1.6773e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3541 - val_accuracy: 0.9473 - val_balanced_acc: 0.9443 - lr: 5.0000e-04\n",
            "Epoch 27/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.6292e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 27: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.6292e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3554 - val_accuracy: 0.9474 - val_balanced_acc: 0.9445 - lr: 5.0000e-04\n",
            "Epoch 28/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.5783e-04 - accuracy: 1.0000 - balanced_acc: 0.9990\n",
            "Epoch 28: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.5783e-04 - accuracy: 1.0000 - balanced_acc: 0.9990 - val_loss: 0.3570 - val_accuracy: 0.9473 - val_balanced_acc: 0.9444 - lr: 5.0000e-04\n",
            "Epoch 29/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.5353e-04 - accuracy: 1.0000 - balanced_acc: 0.9994\n",
            "Epoch 29: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.5353e-04 - accuracy: 1.0000 - balanced_acc: 0.9994 - val_loss: 0.3583 - val_accuracy: 0.9475 - val_balanced_acc: 0.9446 - lr: 5.0000e-04\n",
            "Epoch 30/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.4676e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 30: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.4676e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3598 - val_accuracy: 0.9474 - val_balanced_acc: 0.9445 - lr: 5.0000e-04\n",
            "Epoch 31/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.4407e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 31: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 1.4407e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.3611 - val_accuracy: 0.9471 - val_balanced_acc: 0.9444 - lr: 5.0000e-04\n",
            "Epoch 32/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.4218e-04 - accuracy: 1.0000 - balanced_acc: 0.9992\n",
            "Epoch 32: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.4218e-04 - accuracy: 1.0000 - balanced_acc: 0.9992 - val_loss: 0.3626 - val_accuracy: 0.9473 - val_balanced_acc: 0.9445 - lr: 5.0000e-04\n",
            "Epoch 33/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3647e-04 - accuracy: 1.0000 - balanced_acc: 0.9982\n",
            "Epoch 33: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 352ms/step - loss: 1.3647e-04 - accuracy: 1.0000 - balanced_acc: 0.9982 - val_loss: 0.3641 - val_accuracy: 0.9473 - val_balanced_acc: 0.9445 - lr: 5.0000e-04\n",
            "Epoch 34/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3204e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 34: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.3204e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3652 - val_accuracy: 0.9470 - val_balanced_acc: 0.9440 - lr: 5.0000e-04\n",
            "Epoch 35/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2687e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 35: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 352ms/step - loss: 1.2687e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3658 - val_accuracy: 0.9472 - val_balanced_acc: 0.9443 - lr: 2.5000e-04\n",
            "Epoch 36/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2620e-04 - accuracy: 1.0000 - balanced_acc: 0.9992\n",
            "Epoch 36: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.2620e-04 - accuracy: 1.0000 - balanced_acc: 0.9992 - val_loss: 0.3665 - val_accuracy: 0.9473 - val_balanced_acc: 0.9445 - lr: 2.5000e-04\n",
            "Epoch 37/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2529e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 37: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.2529e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.3671 - val_accuracy: 0.9473 - val_balanced_acc: 0.9444 - lr: 2.5000e-04\n",
            "Epoch 38/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2295e-04 - accuracy: 1.0000 - balanced_acc: 0.9983\n",
            "Epoch 38: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 1.2295e-04 - accuracy: 1.0000 - balanced_acc: 0.9983 - val_loss: 0.3678 - val_accuracy: 0.9471 - val_balanced_acc: 0.9441 - lr: 2.5000e-04\n",
            "Epoch 39/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2146e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 39: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 1.2146e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3685 - val_accuracy: 0.9470 - val_balanced_acc: 0.9441 - lr: 2.5000e-04\n",
            "Epoch 40/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2033e-04 - accuracy: 1.0000 - balanced_acc: 0.9995\n",
            "Epoch 40: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 353ms/step - loss: 1.2033e-04 - accuracy: 1.0000 - balanced_acc: 0.9995 - val_loss: 0.3689 - val_accuracy: 0.9471 - val_balanced_acc: 0.9441 - lr: 2.5000e-04\n",
            "Epoch 41/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1739e-04 - accuracy: 1.0000 - balanced_acc: 0.9989\n",
            "Epoch 41: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 275s 352ms/step - loss: 1.1739e-04 - accuracy: 1.0000 - balanced_acc: 0.9989 - val_loss: 0.3695 - val_accuracy: 0.9470 - val_balanced_acc: 0.9440 - lr: 2.5000e-04\n",
            "Epoch 42/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1818e-04 - accuracy: 1.0000 - balanced_acc: 0.9987\n",
            "Epoch 42: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 276s 354ms/step - loss: 1.1818e-04 - accuracy: 1.0000 - balanced_acc: 0.9987 - val_loss: 0.3700 - val_accuracy: 0.9471 - val_balanced_acc: 0.9443 - lr: 2.5000e-04\n",
            "Epoch 43/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1518e-04 - accuracy: 1.0000 - balanced_acc: 0.9988\n",
            "Epoch 43: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 277s 354ms/step - loss: 1.1518e-04 - accuracy: 1.0000 - balanced_acc: 0.9988 - val_loss: 0.3708 - val_accuracy: 0.9470 - val_balanced_acc: 0.9440 - lr: 2.5000e-04\n",
            "Epoch 44/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1309e-04 - accuracy: 1.0000 - balanced_acc: 0.9985\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 44: val_balanced_acc did not improve from 0.94492\n",
            "781/781 [==============================] - 277s 354ms/step - loss: 1.1309e-04 - accuracy: 1.0000 - balanced_acc: 0.9985 - val_loss: 0.3715 - val_accuracy: 0.9472 - val_balanced_acc: 0.9442 - lr: 2.5000e-04\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = define_base_model()\n",
        "hst = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), verbose=1,\n",
        "                    steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n",
        "# learning curves\n",
        "summarize_diagnostics(hst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwLiXUSG0IZ"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "#hst = model.fit(train_data_batches,\n",
        "#                    epochs = EPOCHS, validation_data = valid_data_batches,      \n",
        "                    #steps_per_epoch=X_train.shape[0] // BATCH_SIZE, \n",
        "#                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vXnW3lmCgln3",
        "outputId": "9576629b-fdf5-45ea-b568-1d3484c2c665"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst.history['accuracy'])\n",
        "plt.plot(hst.history['balanced_acc'])\n",
        "plt.plot(hst.history['val_accuracy'])\n",
        "plt.plot(hst.history['val_balanced_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'train balanced acc.', 'val. accuracy', 'val. balanced acc.'], loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgjmi-4UIT-"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SPz8NH1Oylv9"
      },
      "outputs": [],
      "source": [
        "#save last model\n",
        "model.save(last_model_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lS3ewyxO_anU",
        "outputId": "765575ce-dd12-42e4-d80a-15fca4fed32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on training 1.0\n",
            "balanced accuracy on training 1.0\n",
            "accuracy on validation 0.9472\n",
            "balanced accuracy on validation 0.9471999999999999\n",
            "Score on val data:  (0.9471644703295767, 0.9471999999999999, 0.9471540426871196, None)\n"
          ]
        }
      ],
      "source": [
        "last_model = load_model(last_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = last_model.predict(X_train)\n",
        "y_val_pred = last_model.predict(X_val)\n",
        "\n",
        "#print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W3IyWjdGG4Xq",
        "outputId": "acba118a-34ff-42b7-aa35-14b55f39d0b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on training 1.0\n",
            "balanced accuracy on training 1.0\n",
            "accuracy on validation 0.9474\n",
            "balanced accuracy on validation 0.9474\n",
            "Score on val data:  (0.9473396365973054, 0.9474, 0.9473403871541564, None)\n"
          ]
        }
      ],
      "source": [
        "best_model = load_model(best_model_fpath, custom_objects={'balanced_acc' : balanced_acc})\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print('accuracy on training',accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('balanced accuracy on training',balanced_accuracy_score(np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)))\n",
        "print('accuracy on validation',accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('balanced accuracy on validation',balanced_accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1)))\n",
        "print('Score on val data: ',precision_recall_fscore_support(np.argmax(y_val, axis=1), np.argmax(y_val_pred, axis=1), average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRWiTnO0MGh"
      },
      "source": [
        "#Cut-off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tGnCoIdLyDHS",
        "outputId": "8674cfd9-9cd4-419f-b964-c93037a39f95"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5810551130ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'AKIEC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BCC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BKL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MEL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NV'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VASC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                     \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m                     \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m                 )\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (50000, 10), indices imply (50000, 7)"
          ]
        }
      ],
      "source": [
        "df_train_pred = pd.DataFrame(y_train_pred, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QdyCbloQyWTC"
      },
      "outputs": [],
      "source": [
        "numbers = [float(x)/20 for x in range(11)]\n",
        "\n",
        "for i in numbers:\n",
        "    df_train_pred[i]= df_train_pred.MEL.map(lambda x: 1 if x > i else 0)\n",
        "df_train_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4SQsRx73kgk"
      },
      "outputs": [],
      "source": [
        "y_train_true= [1 if x == 4 else 0 for x in np.argmax(y_train, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QcUISWFi0J05"
      },
      "outputs": [],
      "source": [
        "num = [0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
        "cutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n",
        "for i in num:\n",
        "    cm1 = confusion_matrix(y_train_true, df_train_pred[i])\n",
        "    total1=sum(sum(cm1))\n",
        "    Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "    Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "    Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "    cutoff_df.loc[i] =[ i ,Accuracy,Sensitivity,Specificity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W31LSzov1tCt"
      },
      "outputs": [],
      "source": [
        "cutoff_df[['Accuracy','Sensitivity','Specificity']].plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CIKT94Jqye"
      },
      "outputs": [],
      "source": [
        "i = 0.05\n",
        "cm1 = confusion_matrix(y_train_true, df_train_pred[i])\n",
        "total1=sum(sum(cm1))\n",
        "Accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
        "Specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "Sensitivity = cm1[1,1]/(cm1[1,0]+cm1[1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U2tkFebL_VC"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ', Accuracy)\n",
        "print('Sensitivity: ', Sensitivity)\n",
        "print('Specificity: ', Specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK4zbtoaAaC"
      },
      "source": [
        "#Confusion Metric on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkPOFLehOmFg"
      },
      "outputs": [],
      "source": [
        "#change melanoma flag back to 4\n",
        "df_train_pred[df_train_pred[i] == 1] = 4\n",
        "#decode one-hot y_val_pred while use cut-off melanoma data\n",
        "condition = df_train_pred[i] == 4\n",
        "y_train_pred2 = np.where(condition, df_train_pred[i], np.argmax(y_train_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOVl6dWlTDLo"
      },
      "outputs": [],
      "source": [
        "print('Accuracy: ',accuracy_score(np.argmax(y_train, axis=1), y_train_pred2))\n",
        "print('Balanced accuracy: ',balanced_accuracy_score(np.argmax(y_train, axis=1), y_train_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqvYutTKRhR_"
      },
      "outputs": [],
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(np.argmax(y_train, axis=1), y_train_pred2)\n",
        "print(cf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVtvW3YeaLlC"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(cf_matrix / cf_matrix.sum(axis=1, keepdims=True), annot=True, \n",
        "            cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix \\n');\n",
        "ax.set_xlabel('\\nPredicted')\n",
        "ax.set_ylabel('Actual ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "ax.yaxis.set_ticklabels(['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (15,3)\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-1yjWGeKs7"
      },
      "outputs": [],
      "source": [
        "# ordered count of rows per unique label\n",
        "#labels_count = df_val['Labels'].value_counts().sort_index()\n",
        "\n",
        "#f = plt.figure(figsize=(15, 6))\n",
        "#s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "#s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K908bbiYwbS"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMY2yvMYxsC"
      },
      "outputs": [],
      "source": [
        "dir_test = '/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Test_Input/'\n",
        "filepaths = sorted( filter( lambda x: (os.path.isfile(os.path.join(dir_test, x))) and (x.endswith('.jpg')),\n",
        "                        os.listdir(dir_test) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ic95mefkpG3"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(filepaths, columns =['image'])\n",
        "df_test['FilePaths'] = dir_test + df_test['image']\n",
        "#df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBa1TxPuY8ni"
      },
      "outputs": [],
      "source": [
        "df_test['image_px'] = df_test['FilePaths'].map(lambda x: np.asarray(Image.open(x).resize(IMG_SIZE)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60LYAT7VsNOZ",
        "outputId": "5ebab6bb-9eb5-4551-b881-27fba5c64e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1512, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "X_test = np.asarray(df_test['image_px'].tolist())\n",
        "print(np.array(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXnnIIwC4cHE"
      },
      "outputs": [],
      "source": [
        "#preprocess\n",
        "X_test = preprocess_image_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7ml90JZ8FK"
      },
      "source": [
        "Calculate y_pred from training and testing for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIX0AmEFNv3Y",
        "outputId": "7af3e9c1-d316-485b-d84d-5adbf3ef88e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y_pred2 (1512, 7)\n"
          ]
        }
      ],
      "source": [
        "# predicting\n",
        "#CHANGE THE MODEL IF NECESSARY\n",
        "Y_pred2 = best_model.predict(X_test)\n",
        "print(\"Y_pred2\", Y_pred2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeArO5CtxGb"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.DataFrame(Y_pred2, columns = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'])\n",
        "df_pred['image'] = df_test['FilePaths'].map(lambda x: x.replace(dir_test, '').replace('.jpg', ''))\n",
        "df_pred = df_pred[['image', 'MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']]\n",
        "df_pred.set_index(\"image\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ynyd8PjT589"
      },
      "outputs": [],
      "source": [
        "#update MEL data using cut-off value\n",
        "df_pred.MEL[df_pred.MEL > i] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjRdONoQVMq0"
      },
      "outputs": [],
      "source": [
        "df_pred.loc[df_pred.MEL > i, ['NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnjc3RJ0e4T"
      },
      "outputs": [],
      "source": [
        "df_pred.to_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/response_SMOTEOversampling_cut-off_InceptionV3.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRGeofw-8tK"
      },
      "source": [
        "#Load ISIC 2018 Challange Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3P7IjyLuZGY"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = load_isic2018_dataset(train_under_frac = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "2IncA-_o_n5w",
        "outputId": "6db08704-addd-42d9-9371-4d805a2db101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Text(0, 0, 'DF'),\n",
              " Text(0, 0, 'VASC'),\n",
              " Text(0, 0, 'AKIEC'),\n",
              " Text(0, 0, 'BCC'),\n",
              " Text(0, 0, 'BKL'),\n",
              " Text(0, 0, 'MEL'),\n",
              " Text(0, 0, 'NV')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ordered count of rows per unique label\n",
        "labels_count = y_train.value_counts(ascending=True)\n",
        "\n",
        "f = plt.figure(figsize=(15, 6))\n",
        "s = sns.barplot(x=labels_count.index,y=labels_count.values)\n",
        "s.set_xticklabels(s.get_xticklabels(), rotation = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKMKSb4Bkym"
      },
      "source": [
        "Plot 3 images per label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdnVuqbFBW3K"
      },
      "outputs": [],
      "source": [
        "def plot_images_per_label(df, label, cols: int, size: tuple):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=size)\n",
        "\n",
        "    cntMax = cols\n",
        "    cntCur = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if(y_train == label and cntCur < cntMax):\n",
        "            axs[cntCur].imshow(plt.imread(df.FilePaths[index]))\n",
        "            axs[cntCur].set_title(df.Labels[index])\n",
        "\n",
        "            cntCur += 1\n",
        "        else:\n",
        "            if(cntCur >= cntMax):\n",
        "                break\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# unique labels\n",
        "#labels = sorted(df_train['Labels'].unique())\n",
        "#for label in range(7):\n",
        "#    plot_images_per_label(y_train, 3, (12,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKKrNacAZtl"
      },
      "source": [
        "Drop duplicate images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERwfyPDHP-zC"
      },
      "outputs": [],
      "source": [
        "#df_group = pd.read_csv('/content/drive/MyDrive/PHD/Datasets/isic2018/ISIC2018_Task3_Training_LesionGroupings.csv') \n",
        "#df_train = df_train.set_index('image').join(df_group.set_index('image'))\n",
        "#df_train = df_train.drop_duplicates(subset=['lesion_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXx28B9yGu"
      },
      "source": [
        "#DeepSMOTE Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmX_Uqbmj-tN"
      },
      "outputs": [],
      "source": [
        "from numpy import moveaxis\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_el = np.inf\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 3#1    # number of channels in the input data \n",
        "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 300       # how many epochs to run for\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "args['patience'] = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NydOdPMajEfT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        \n",
        "        # convolutional filters, work excellent with image data\n",
        "        # [(WK+2P)/S]+1\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(7, stride=7),\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),# 16\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False), # 8\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),# 4\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 0, bias=False),#14\n",
        "            nn.BatchNorm2d(self.dim_h * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 2**3 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        # H_out = (H_in1)*stride[0]  2padding[0] + dilation[0](kernel_size[0]1) + output_padding[0] + 1\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4), #10\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4), #13\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h, 4),# 16\n",
        "            nn.BatchNorm2d(self.dim_h),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h, 3, 4, 2, 1),# 32\n",
        "            nn.UpsamplingBilinear2d(scale_factor=7),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 2**3, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def biased_get_class(X, y, c):\n",
        "    \n",
        "    xbeg = X[y == c]\n",
        "    ybeg = y[y == c]\n",
        "    \n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "    n_neigh = 5\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "def DeepSMOTE_train(X_train, y_train, one_hot = False):\n",
        "  from torch.utils.data import TensorDataset\n",
        "  import os\n",
        "\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #X_train = X_train.astype('float32') / 255.\n",
        "  \n",
        "  batch_size = args['batch_size']\n",
        "  patience = args['patience']\n",
        "  encoder = Encoder(args)\n",
        "  decoder = Decoder(args)\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print(device)\n",
        "  decoder = decoder.to(device)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "  #decoder loss function\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  num_workers = 0\n",
        "\n",
        "  #torch.Tensor returns float so if want long then use torch.tensor\n",
        "  tensor_x = torch.from_numpy(X_train.copy())#torch.Tensor(X_train)\n",
        "  tensor_y = torch.tensor(y_train,dtype=torch.long)\n",
        "  mnist_bal = TensorDataset(tensor_x,tensor_y) \n",
        "  train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
        "      batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
        "\n",
        "  best_loss = np.inf\n",
        "\n",
        "  enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
        "  dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
        "\n",
        "  for epoch in range(args['epochs']):\n",
        "      train_loss = 0.0\n",
        "      tmse_loss = 0.0\n",
        "      tdiscr_loss = 0.0\n",
        "      # train for one epoch -- set nets to train mode\n",
        "      encoder.train()\n",
        "      decoder.train()\n",
        "  \n",
        "      for images,labs in train_loader:\n",
        "      \n",
        "          # zero gradients for each batch\n",
        "          encoder.zero_grad()\n",
        "          decoder.zero_grad()\n",
        "          images, labs = images.to(device), labs.to(device)\n",
        "          labsn = labs.detach().cpu().numpy()\n",
        "#            print('images shape', images.shape)\n",
        "          # run images\n",
        "          z_hat = encoder(images)\n",
        "#            print('images shape after encoding', z_hat.shape)\n",
        "      \n",
        "          x_hat = decoder(z_hat) #decoder outputs tanh\n",
        "#            print('images shape after decoding', x_hat.shape)\n",
        "          mse = criterion(x_hat,images)\n",
        "                  \n",
        "          resx = []\n",
        "          resy = []\n",
        "      \n",
        "          tc = np.random.choice(num_classes,1)\n",
        "          #tc = 9\n",
        "          xbeg = X_train[y_train == tc]\n",
        "          ybeg = y_train[y_train == tc] \n",
        "          xlen = len(xbeg)\n",
        "          nsamp = min(xlen, 100)\n",
        "          ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
        "          xclass = xbeg[ind]\n",
        "          yclass = ybeg[ind]\n",
        "      \n",
        "          xclen = len(xclass)\n",
        "          xcminus = np.arange(1,xclen)\n",
        "          \n",
        "          xcplus = np.append(xcminus,0)\n",
        "          xcnew = (xclass[[xcplus],:])\n",
        "          xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
        "      \n",
        "          xcnew = torch.Tensor(xcnew)\n",
        "          xcnew = xcnew.to(device)\n",
        "      \n",
        "          #encode xclass to feature space\n",
        "          xclass = torch.Tensor(xclass)\n",
        "          xclass = xclass.to(device)\n",
        "          xclass = encoder(xclass)\n",
        "      \n",
        "          xclass = xclass.detach().cpu().numpy()\n",
        "      \n",
        "          xc_enc = (xclass[[xcplus],:])\n",
        "          xc_enc = np.squeeze(xc_enc)\n",
        "      \n",
        "          xc_enc = torch.Tensor(xc_enc)\n",
        "          xc_enc = xc_enc.to(device)\n",
        "          \n",
        "          ximg = decoder(xc_enc)\n",
        "          \n",
        "          mse2 = criterion(ximg,xcnew)\n",
        "      \n",
        "          comb_loss = mse2 + mse\n",
        "          comb_loss.backward()\n",
        "      \n",
        "          enc_optim.step()\n",
        "          dec_optim.step()\n",
        "      \n",
        "          train_loss += comb_loss.item()*images.size(0)\n",
        "          tmse_loss += mse.item()*images.size(0)\n",
        "          tdiscr_loss += mse2.item()*images.size(0)\n",
        "\n",
        "      train_loss = train_loss/len(train_loader)\n",
        "      tmse_loss = tmse_loss/len(train_loader)\n",
        "      tdiscr_loss = tdiscr_loss/len(train_loader)\n",
        "      print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
        "              train_loss,tmse_loss,tdiscr_loss))\n",
        "      \n",
        "  \n",
        "  \n",
        "      #store the best encoder and decoder models\n",
        "      #here, /crs5 is a reference to 5 way cross validation, but is not\n",
        "      #necessary for illustration purposes\n",
        "      if train_loss < best_loss:\n",
        "          print('Saving..')\n",
        "          patience = args['patience']\n",
        "          path_enc = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_enc.pth'\n",
        "          path_dec = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/bst_dec.pth'\n",
        "        \n",
        "          torch.save(encoder.state_dict(), path_enc)\n",
        "          torch.save(decoder.state_dict(), path_dec)\n",
        "  \n",
        "          best_loss = train_loss\n",
        "      else:\n",
        "          patience = patience - 1\n",
        "\n",
        "      if patience == 0:\n",
        "          print('Out of patience. \\n')\n",
        "          break\n",
        "\n",
        "def DeepSMOTE_Data(X_train, y_train, one_hot = False):\n",
        "  batch_size = args['batch_size']\n",
        "  max_el = np.max(X_train)\n",
        "  X_train = X_train / max_el\n",
        "  X_train = moveaxis(X_train, 3, 1)\n",
        "  if one_hot:\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "  #Generate artificial images\n",
        "  import torch\n",
        "  np.printoptions(precision=5,suppress=True)\n",
        "\n",
        "  #path on the computer where the models are stored\n",
        "  modpth = '/content/drive/MyDrive/PHD/Model/DeepSMOTE/32/'\n",
        "\n",
        "  path_enc = modpth + '/bst_enc.pth'\n",
        "  path_dec = modpth + '/bst_dec.pth'\n",
        "  \n",
        "  train_on_gpu = torch.cuda.is_available()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  encoder = Encoder(args)\n",
        "  encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  decoder = Decoder(args)\n",
        "  decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
        "  decoder = decoder.to(device)\n",
        "\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  resx = []\n",
        "  resy = []\n",
        "  \n",
        "  counter = Counter(y_train)\n",
        "  counter = sorted(counter.items())\n",
        "  counter = [value for _, value in counter]\n",
        "\n",
        "  for i in range(num_classes):\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      xclass, yclass = biased_get_class(X_train, y_train, i)\n",
        "      #encode xclass to feature space\n",
        "      xclass = torch.Tensor(xclass)\n",
        "      xclass = xclass.to(device)\n",
        "      xclass = encoder(xclass)\n",
        "          \n",
        "      xclass = xclass.detach().cpu().numpy()\n",
        "      n = np.max(counter) - counter[i]\n",
        "      if n == 0:\n",
        "        continue\n",
        "#        resx2 = []\n",
        "#        resy2 = []\n",
        "#        for j in range(batch_size, n+batch_size+1, batch_size):\n",
        "#          if j <= n:\n",
        "#            batch_size_max = batch_size\n",
        "#          elif n % batch_size != 0:\n",
        "#            batch_size_max = n%batch_size\n",
        "#          else:\n",
        "#            break\n",
        "#          xsamp, ysamp = G_SM(xclass,yclass,batch_size_max,i)\n",
        "      xsamp, ysamp = G_SM(xclass,yclass,n,i)\n",
        "      ysamp = np.array(ysamp)\n",
        "  \n",
        "      \"\"\"to generate samples for resnet\"\"\"   \n",
        "      xsamp = torch.Tensor(xsamp)\n",
        "      xsamp = xsamp.to(device)\n",
        "      ximg = decoder(xsamp)\n",
        "\n",
        "      ximn = ximg.detach().cpu().numpy()\n",
        "#        resx2.append(ximn)\n",
        "#        resy2.append(ysamp)\n",
        "#        \n",
        "#        resx2 = np.vstack(resx2)\n",
        "#        resy2 = np.hstack(resy2)\n",
        "      resx.append(ximn)\n",
        "      resy.append(ysamp)\n",
        "  \n",
        "  resx1 = np.vstack(resx)\n",
        "  resy1 = np.hstack(resy)\n",
        "  resx1 = resx1.reshape(resx1.shape[0],-1)\n",
        "  X_train = X_train.reshape(X_train.shape[0],-1)\n",
        "  X_train = np.vstack((resx1,X_train))\n",
        "  y_train = np.hstack((resy1,y_train))\n",
        "  y_train = to_categorical(y_train)\n",
        "  X_train = X_train.reshape(-1, 3, IMAGE_W, IMAGE_H)\n",
        "  X_train = moveaxis(X_train, 1, 3)\n",
        "  X_train = X_train * max_el\n",
        "  return X_train, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jrJ33lUDkCM"
      },
      "source": [
        "#Split dataset to train and val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6qneWL_Bs2U",
        "outputId": "701f956e-871c-49f2-a88d-08d5ae5a9221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:  (11261, 224, 224, 3)\n",
            "Remaining Data:  (2816, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# stratified train and rem (20%) datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=1)\n",
        "\n",
        "print('Train Data: ', X_train.shape)\n",
        "print('Remaining Data: ', X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kef4r_zxjgk"
      },
      "outputs": [],
      "source": [
        "#Data Augmentation\n",
        "dataaugment = ImageDataGenerator(\n",
        "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "        shear_range = 10) \n",
        "\n",
        "dataaugment.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PgksTFkOAq"
      },
      "source": [
        "#Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr1jnSM7yzJc"
      },
      "outputs": [],
      "source": [
        "limit = 171\n",
        "for layer in model.layers[:limit]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[limit:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "optimizer_SGD = SGD(learning_rate=0.0001, momentum=0.9)\n",
        "model.compile(optimizer = optimizer_SGD , loss = \"categorical_crossentropy\", metrics=['accuracy', balanced_acc])\n",
        "hst2 = model.fit(train_data_batches,\n",
        "                    epochs = EPOCHS, validation_data = valid_data_batches,\n",
        "                    callbacks=[learning_rate_reduction,early_stopping_monitor, mc])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO1aAQBmiy0K"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hst2.history['balanced_acc'])\n",
        "plt.plot(hst2.history['val_balanced_acc'])\n",
        "plt.title('model balance_acc after tunning')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3K908bbiYwbS",
        "RcRGeofw-8tK",
        "cNBXx28B9yGu",
        "0jrJ33lUDkCM",
        "B2PgksTFkOAq"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}